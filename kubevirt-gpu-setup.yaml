# KubeVirt GPU Passthrough Setup
# This file configures KubeVirt for GPU passthrough and Backend.AI Agent VMs

---
# KubeVirt CRD Installation
apiVersion: kubevirt.io/v1
kind: KubeVirt
metadata:
  name: kubevirt
  namespace: kubevirt
spec:
  certificateRotateStrategy: {}
  configuration:
    developerConfiguration:
      featureGates:
        - GPU
        - DisableMDEVConfiguration
        - HypervStrictCheck
        - VirtIOFS
        - HostDevices
    permittedHostDevices:
      pciHostDevices:
      # NVIDIA A10 GPU
      - pciVendorSelector: 10DE:2236
        resourceName: nvidia.com/GA102GL_A10
        externalResourceProvider: true
      # NVIDIA Tesla T4 GPU  
      - pciVendorSelector: 10DE:1EB8
        resourceName: nvidia.com/TU104GL_Tesla_T4
        externalResourceProvider: true
      # NVIDIA RTX A6000 GPU
      - pciVendorSelector: 10DE:2230
        resourceName: nvidia.com/GA102_RTX_A6000
        externalResourceProvider: true
      # NVIDIA V100 GPU
      - pciVendorSelector: 10DE:1DB4
        resourceName: nvidia.com/GV100GL_Tesla_V100
        externalResourceProvider: true
    network:
      defaultNetworkInterface: bridge
      permitBridgeInterfaceOnPodNetwork: true
    supportedGuestAgentVersions:
    - 6.1.0

---
# Namespace for KubeVirt
apiVersion: v1
kind: Namespace
metadata:
  name: kubevirt

---
# Namespace for Backend.AI GPU VMs
apiVersion: v1
kind: Namespace
metadata:
  name: backend-ai-gpu
  labels:
    app.kubernetes.io/name: backend-ai-gpu
    app.kubernetes.io/part-of: backend-ai

---
# NVIDIA GPU Device Plugin for KubeVirt
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nvidia-kubevirt-gpu-device-plugin
  namespace: kubevirt
  labels:
    app: nvidia-kubevirt-gpu-device-plugin
spec:
  selector:
    matchLabels:
      name: nvidia-kubevirt-gpu-device-plugin
  template:
    metadata:
      labels:
        name: nvidia-kubevirt-gpu-device-plugin
    spec:
      nodeSelector:
        nvidia.com/gpu.workload.config: vm-passthrough
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      containers:
      - name: nvidia-kubevirt-gpu-device-plugin
        image: nvidia/kubevirt-gpu-device-plugin:v1.2.4
        imagePullPolicy: Always
        command:
        - nvidia-kubevirt-gpu-device-plugin
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: KUBEVIRT_GPU_DEVICE_PLUGIN_LOG_LEVEL
          value: "4"
        securityContext:
          privileged: true
        volumeMounts:
        - name: device-plugin
          mountPath: /var/lib/kubelet/device-plugins
        - name: dev
          mountPath: /dev
        - name: sys
          mountPath: /sys
        resources:
          requests:
            cpu: 50m
            memory: 50Mi
          limits:
            cpu: 100m
            memory: 100Mi
      volumes:
      - name: device-plugin
        hostPath:
          path: /var/lib/kubelet/device-plugins
      - name: dev
        hostPath:
          path: /dev
      - name: sys
        hostPath:
          path: /sys
      hostNetwork: true
      hostPID: true

---
# Priority Class for GPU VMs
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: gpu-vm-priority
value: 1000
globalDefault: false
description: "Priority class for GPU VMs running Backend.AI Agents"

---
# Network Attachment Definition for GPU VM Management
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: backend-ai-gpu-management-net
  namespace: backend-ai-gpu
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "backend-ai-gpu-management",
      "type": "bridge",
      "bridge": "br-gpu-mgmt",
      "isDefaultGateway": false,
      "ipMasq": false,
      "ipam": {
        "type": "host-local",
        "subnet": "10.244.100.0/24",
        "rangeStart": "10.244.100.10",
        "rangeEnd": "10.244.100.200",
        "routes": [
          {
            "dst": "0.0.0.0/0"
          }
        ],
        "gateway": "10.244.100.1"
      }
    }

---
# Storage Class for GPU VM disks
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gpu-vm-fast-ssd
  annotations:
    storageclass.kubernetes.io/is-default-class: "false"
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
parameters:
  type: local-ssd
  fsType: ext4

---
# ConfigMap for Backend.AI Agent base configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: backend-ai-agent-base-config
  namespace: backend-ai-gpu
data:
  agent.toml: |
    [etcd]
    namespace = "backend.ai"
    endpoints = ["http://backend-ai-etcd.backend-ai.svc.cluster.local:2379"]
    user = ""
    password = ""
    
    [agent]
    backend = "docker"
    rpc-listen-addr = "tcp://0.0.0.0:6011"
    id = "${BACKEND_AI_AGENT_ID}"
    region = "default"
    scaling-group = "gpu-nodes"
    pid-file = "/tmp/backend.ai-agent.pid"
    event-loop = "uvloop"
    
    [container]
    bind-host = "0.0.0.0"
    scratch-type = "hostdir"
    scratch-root = "/var/cache/scratches"
    stats-type = "docker"
    sandbox-type = "docker"
    
    [docker-registry."harbor.backend-ai.local:30002"]
    url = "http://harbor.backend-ai.local:30002"
    username = "admin"
    password = "Harbor12345"
    ssl-verify = false
    
    [logging]
    level = "INFO"
    drivers = ["console"]
    
    [debug]
    enabled = false
    asyncio = false
    
    [resource]
    cuda.enabled = true
    cuda.version = "12.2"
    opencl.enabled = false
    
    [watcher]
    service-addr = "tcp://0.0.0.0:6009"
    
  init-script.sh: |
    #!/bin/bash
    set -e
    
    # Setup GPU environment
    echo "Setting up GPU environment..."
    
    # Install NVIDIA drivers if not present
    if ! nvidia-smi > /dev/null 2>&1; then
        echo "Installing NVIDIA drivers..."
        apt-get update
        apt-get install -y nvidia-driver-535 nvidia-utils-535
    fi
    
    # Install Docker if not present  
    if ! docker --version > /dev/null 2>&1; then
        echo "Installing Docker..."
        curl -fsSL https://get.docker.com | sh
        systemctl enable docker
        systemctl start docker
    fi
    
    # Install NVIDIA Container Toolkit
    if ! nvidia-ctk --version > /dev/null 2>&1; then
        echo "Installing NVIDIA Container Toolkit..."
        distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
        curl -s -L https://nvidia.github.io/libnvidia-container/gpgkey | apt-key add -
        curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
            tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
        apt-get update
        apt-get install -y nvidia-container-toolkit
        
        # Configure Docker daemon
        nvidia-ctk runtime configure --runtime=docker
        systemctl restart docker
    fi
    
    # Test GPU access
    echo "Testing GPU access..."
    nvidia-smi
    docker run --rm --gpus all nvidia/cuda:12.2-base-ubuntu22.04 nvidia-smi
    
    echo "GPU setup completed successfully!"