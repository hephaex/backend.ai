# Backend.AI vLLM ìë™ ë°°í¬ ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

ì´ ê°€ì´ë“œëŠ” Backend.AI APIë¥¼ ì‚¬ìš©í•˜ì—¬ Hugging Face ëª¨ë¸ì„ vLLMìœ¼ë¡œ ìë™ ë°°í¬í•˜ëŠ” ì™„ì „í•œ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤. GPU ë¦¬ì†ŒìŠ¤ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ì„œë¹™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Backend.AI Platform                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Manager                                                        â”‚
â”‚  â”œâ”€â”€ API Gateway (í¬íŠ¸ 8081)                                    â”‚
â”‚  â”œâ”€â”€ Session Management                                         â”‚
â”‚  â””â”€â”€ Resource Orchestration                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  GPU Agent Nodes                                               â”‚
â”‚  â”œâ”€â”€ Agent 1 (GPU Compute)                                     â”‚
â”‚  â”‚   â”œâ”€â”€ vLLM Container                                        â”‚
â”‚  â”‚   â”‚   â”œâ”€â”€ Model: HuggingFace Model                         â”‚
â”‚  â”‚   â”‚   â”œâ”€â”€ API Server: OpenAI Compatible                    â”‚
â”‚  â”‚   â”‚   â””â”€â”€ GPU: CUDA Acceleration                           â”‚
â”‚  â”‚   â””â”€â”€ Resources: GPU + CPU + Memory                        â”‚
â”‚  â””â”€â”€ Agent N (Scalable)                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  External Integration                                           â”‚
â”‚  â”œâ”€â”€ HuggingFace Hub (Model Download)                         â”‚
â”‚  â”œâ”€â”€ OpenAI Compatible API                                     â”‚
â”‚  â””â”€â”€ Client Applications                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### 1ë‹¨ê³„: í™˜ê²½ ì„¤ì •

```bash
# Backend.AI ìê²© ì¦ëª… ì„¤ì •
export BACKEND_AI_ENDPOINT="http://localhost:8081"
export BACKEND_AI_ACCESS_KEY="your-access-key"
export BACKEND_AI_SECRET_KEY="your-secret-key"

# HuggingFace í† í° (private ëª¨ë¸ìš©)
export HUGGING_FACE_HUB_TOKEN="your-hf-token"
```

### 2ë‹¨ê³„: ê°„ë‹¨í•œ ëª¨ë¸ ë°°í¬

```bash
# ì‘ì€ ëª¨ë¸ ë°°í¬ (í…ŒìŠ¤íŠ¸ìš©)
./deploy-vllm-model.sh microsoft/DialoGPT-medium

# ì¤‘ê°„ í¬ê¸° ëª¨ë¸ ë°°í¬
./deploy-vllm-model.sh -g 1 -m 16g --wait meta-llama/Llama-2-7b-chat-hf

# ëŒ€í˜• ëª¨ë¸ ë°°í¬ (ë‹¤ì¤‘ GPU)
./deploy-vllm-model.sh -g 4 -m 64g --quantization awq \
    --tensor-parallel-size 4 meta-llama/Llama-2-70b-chat-hf
```

### 3ë‹¨ê³„: API ì‚¬ìš©

```bash
# ë°°í¬ëœ ëª¨ë¸ API í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "prompt": "Hello, how are you?",
    "max_tokens": 100,
    "temperature": 0.7
  }'
```

## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
backend.ai/
â”œâ”€â”€ VLLM_DEPLOYMENT_GUIDE.md           # ì´ ë¬¸ì„œ
â”œâ”€â”€ deploy-vllm-model.sh               # ë©”ì¸ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ vllm-examples.sh                   # ì˜ˆì œ ë° ìœ í‹¸ë¦¬í‹°
â””â”€â”€ temp/
    â””â”€â”€ deploy_vllm_*.py               # ì„ì‹œ Python ìŠ¤í¬ë¦½íŠ¸
```

## ğŸ”§ deploy-vllm-model.sh ì‚¬ìš©ë²•

### ê¸°ë³¸ ë¬¸ë²•

```bash
./deploy-vllm-model.sh [OPTIONS] <huggingface-model-url>
```

### ì£¼ìš” ì˜µì…˜

| ì˜µì…˜ | ì„¤ëª… | ê¸°ë³¸ê°’ | ì˜ˆì‹œ |
|------|------|---------|------|
| `-g, --gpu-count` | GPU ê°œìˆ˜ | 1 | `-g 2` |
| `-m, --memory` | ë©”ëª¨ë¦¬ í• ë‹¹ | 16g | `-m 32g` |
| `-c, --cpu-count` | CPU ì½”ì–´ ìˆ˜ | 4 | `-c 8` |
| `-n, --session-name` | ì„¸ì…˜ ì´ë¦„ | ìë™ìƒì„± | `-n my-llama` |
| `--max-model-len` | ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ | 4096 | `--max-model-len 8192` |
| `--quantization` | ì–‘ìí™” ë°©ë²• | ì—†ìŒ | `--quantization awq` |
| `--tensor-parallel-size` | í…ì„œ ë³‘ë ¬í™” í¬ê¸° | ì—†ìŒ | `--tensor-parallel-size 4` |
| `--huggingface-token` | HF í† í° | í™˜ê²½ë³€ìˆ˜ | `--huggingface-token hf_xxx` |
| `--wait` | ë°°í¬ ì™„ë£Œ ëŒ€ê¸° | false | `--wait` |
| `--dry-run` | ì‹¤í–‰í•˜ì§€ ì•Šê³  ì„¤ì •ë§Œ í™•ì¸ | false | `--dry-run` |

### ì‚¬ìš© ì˜ˆì‹œ

#### 1. ê¸°ë³¸ ë°°í¬

```bash
# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ëª¨ë¸ ë°°í¬
./deploy-vllm-model.sh microsoft/DialoGPT-medium
```

#### 2. ì»¤ìŠ¤í…€ ë¦¬ì†ŒìŠ¤ ë°°í¬

```bash
# 2ê°œ GPU, 32GB ë©”ëª¨ë¦¬ë¡œ ë°°í¬
./deploy-vllm-model.sh -g 2 -m 32g -c 8 meta-llama/Llama-2-13b-chat-hf
```

#### 3. ì–‘ìí™”ëœ ëŒ€í˜• ëª¨ë¸ ë°°í¬

```bash
# AWQ ì–‘ìí™”ë¡œ 4ê°œ GPUì— ë¶„ì‚° ë°°í¬
./deploy-vllm-model.sh \
    --gpu-count 4 \
    --memory 64g \
    --cpu-count 16 \
    --quantization awq \
    --tensor-parallel-size 4 \
    --max-model-len 8192 \
    --wait \
    meta-llama/Llama-2-70b-chat-hf
```

#### 4. Private ëª¨ë¸ ë°°í¬

```bash
# HuggingFace í† í°ìœ¼ë¡œ private ëª¨ë¸ ë°°í¬
./deploy-vllm-model.sh \
    --huggingface-token hf_xxxxxxxxxxxxxxxx \
    --gpu-count 2 \
    --memory 24g \
    --wait \
    your-org/private-llama-model
```

#### 5. ì„¤ì • í™•ì¸ (Dry Run)

```bash
# ì‹¤ì œ ë°°í¬í•˜ì§€ ì•Šê³  ì„¤ì •ë§Œ í™•ì¸
./deploy-vllm-model.sh --dry-run microsoft/DialoGPT-medium
```

## ğŸ¯ vllm-examples.sh ìœ í‹¸ë¦¬í‹°

### ëŒ€í™”í˜• ë©”ë‰´ ì‹¤í–‰

```bash
./vllm-examples.sh
```

### ì§ì ‘ ëª…ë ¹ì–´ ì‚¬ìš©

```bash
# í™œì„± ì„¸ì…˜ ëª©ë¡ ë³´ê¸°
./vllm-examples.sh list

# ì„¸ì…˜ ì¢…ë£Œ
./vllm-examples.sh kill session-name

# ì„¸ì…˜ ë¡œê·¸ ë³´ê¸°
./vllm-examples.sh logs session-name

# API í…ŒìŠ¤íŠ¸
./vllm-examples.sh test http://localhost:8000

# ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
./vllm-examples.sh monitor
```

## ğŸ“Š ì§€ì› ëª¨ë¸ ìœ í˜•

### 1. í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸

| ëª¨ë¸ | ê¶Œì¥ GPU | ë©”ëª¨ë¦¬ | ì˜ˆì‹œ ëª…ë ¹ì–´ |
|------|----------|--------|-------------|
| GPT-2 | 1 | 4g | `./deploy-vllm-model.sh gpt2` |
| DialoGPT-medium | 1 | 8g | `./deploy-vllm-model.sh microsoft/DialoGPT-medium` |
| DialoGPT-large | 1 | 16g | `./deploy-vllm-model.sh -m 16g microsoft/DialoGPT-large` |

### 2. ì¤‘ê°„ í¬ê¸° ëª¨ë¸

| ëª¨ë¸ | ê¶Œì¥ GPU | ë©”ëª¨ë¦¬ | ì˜ˆì‹œ ëª…ë ¹ì–´ |
|------|----------|--------|-------------|
| Llama-2-7B | 1 | 16g | `./deploy-vllm-model.sh meta-llama/Llama-2-7b-chat-hf` |
| Llama-2-13B | 2 | 32g | `./deploy-vllm-model.sh -g 2 -m 32g meta-llama/Llama-2-13b-chat-hf` |
| Mistral-7B | 1 | 16g | `./deploy-vllm-model.sh mistralai/Mistral-7B-Instruct-v0.1` |

### 3. ëŒ€í˜• ëª¨ë¸

| ëª¨ë¸ | ê¶Œì¥ GPU | ë©”ëª¨ë¦¬ | ì–‘ìí™” | ì˜ˆì‹œ ëª…ë ¹ì–´ |
|------|----------|--------|--------|-------------|
| Llama-2-70B | 4 | 64g | AWQ | `./deploy-vllm-model.sh -g 4 -m 64g --quantization awq --tensor-parallel-size 4 meta-llama/Llama-2-70b-chat-hf` |
| CodeLlama-34B | 2 | 48g | GPTQ | `./deploy-vllm-model.sh -g 2 -m 48g --quantization gptq codellama/CodeLlama-34b-Instruct-hf` |

### 4. íŠ¹ìˆ˜ ëª©ì  ëª¨ë¸

| ëª¨ë¸ | ìš©ë„ | ê¶Œì¥ ì„¤ì • | ì˜ˆì‹œ ëª…ë ¹ì–´ |
|------|------|-----------|-------------|
| Code Llama | ì½”ë“œ ìƒì„± | 1-2 GPU, 24g | `./deploy-vllm-model.sh -g 2 -m 24g codellama/CodeLlama-13b-Instruct-hf` |
| Vicuna | ì±„íŒ… | 1 GPU, 16g | `./deploy-vllm-model.sh -m 16g lmsys/vicuna-13b-v1.5` |
| WizardCoder | ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ | 1 GPU, 16g | `./deploy-vllm-model.sh -m 16g WizardLM/WizardCoder-15B-V1.0` |

## ğŸ” íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œ

#### 1. ì¸ì¦ ì˜¤ë¥˜

```bash
# ì˜¤ë¥˜: Backend.AI credentials are required
export BACKEND_AI_ACCESS_KEY="your-access-key"
export BACKEND_AI_SECRET_KEY="your-secret-key"
```

#### 2. GPU ë¦¬ì†ŒìŠ¤ ë¶€ì¡±

```bash
# ì˜¤ë¥˜: Insufficient GPU resources
# í•´ê²°: GPU ê°œìˆ˜ ì¤„ì´ê±°ë‚˜ ëŒ€ê¸° ì¤‘ì¸ ì„¸ì…˜ ì¢…ë£Œ
./vllm-examples.sh list
./vllm-examples.sh kill session-name
```

#### 3. ë©”ëª¨ë¦¬ ë¶€ì¡±

```bash
# ì˜¤ë¥˜: Out of memory
# í•´ê²°: ë©”ëª¨ë¦¬ í• ë‹¹ ëŠ˜ë¦¬ê±°ë‚˜ ì–‘ìí™” ì‚¬ìš©
./deploy-vllm-model.sh -m 32g --quantization awq model-name
```

#### 4. HuggingFace ëª¨ë¸ ì ‘ê·¼ ì˜¤ë¥˜

```bash
# ì˜¤ë¥˜: Repository not found or private
# í•´ê²°: HuggingFace í† í° ì„¤ì •
export HUGGING_FACE_HUB_TOKEN="hf_xxxxxxxxxxxxxxxx"
./deploy-vllm-model.sh --huggingface-token $HUGGING_FACE_HUB_TOKEN model-name
```

#### 5. ì„¸ì…˜ ì‹œì‘ ì‹¤íŒ¨

```bash
# ë¡œê·¸ í™•ì¸
./vllm-examples.sh logs session-name

# ì„¸ì…˜ ì¬ì‹œì‘
./vllm-examples.sh kill session-name
./deploy-vllm-model.sh model-name
```

### ë””ë²„ê¹… ë„êµ¬

#### 1. ìƒì„¸ ë¡œê·¸ í™œì„±í™”

```bash
# ë””ë²„ê·¸ ëª¨ë“œë¡œ ì‹¤í–‰
./deploy-vllm-model.sh --debug --verbose microsoft/DialoGPT-medium
```

#### 2. Dry Runìœ¼ë¡œ ì„¤ì • í™•ì¸

```bash
# ì‹¤í–‰ ì—†ì´ ì„¤ì • í™•ì¸
./deploy-vllm-model.sh --dry-run --verbose model-name
```

#### 3. ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§

```bash
# GPU ì‚¬ìš©ë¥  í™•ì¸
nvidia-smi

# Backend.AI ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
./vllm-examples.sh monitor
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### 1. GPU í™œìš© ìµœì í™”

```bash
# í…ì„œ ë³‘ë ¬í™”ë¡œ ë‹¤ì¤‘ GPU í™œìš©
./deploy-vllm-model.sh \
    --gpu-count 4 \
    --tensor-parallel-size 4 \
    model-name
```

### 2. ë©”ëª¨ë¦¬ ìµœì í™”

```bash
# ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
./deploy-vllm-model.sh \
    --quantization awq \
    --max-model-len 4096 \
    model-name
```

### 3. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

```bash
# vLLM í™˜ê²½ ë³€ìˆ˜ ì¡°ì • (ê³ ê¸‰ ì‚¬ìš©ììš©)
# ë°°í¬ í›„ ì„¸ì…˜ ë‚´ì—ì„œ ìˆ˜ì • í•„ìš”
export VLLM_USE_MODELSCOPE=false
export VLLM_WORKER_MULTIPROC_METHOD=spawn
```

## ğŸ”„ API ì‚¬ìš© ê°€ì´ë“œ

### 1. OpenAI í˜¸í™˜ API

#### í…ìŠ¤íŠ¸ ì™„ì„±

```bash
curl -X POST http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "prompt": "Explain quantum computing in simple terms:",
    "max_tokens": 200,
    "temperature": 0.7,
    "top_p": 0.9
  }'
```

#### ì±„íŒ… ì™„ì„±

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "messages": [
      {"role": "user", "content": "What is machine learning?"}
    ],
    "max_tokens": 200,
    "temperature": 0.7
  }'
```

#### ëª¨ë¸ ì •ë³´ ì¡°íšŒ

```bash
# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
curl http://localhost:8000/v1/models

# ì„œë²„ ìƒíƒœ í™•ì¸
curl http://localhost:8000/health
```

### 2. Python í´ë¼ì´ì–¸íŠ¸ ì˜ˆì œ

```python
import openai

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
openai.api_base = "http://localhost:8000/v1"
openai.api_key = "sk-dummy"  # vLLMì€ API í‚¤ê°€ í•„ìš” ì—†ìŒ

# í…ìŠ¤íŠ¸ ìƒì„±
response = openai.Completion.create(
    model="meta-llama/Llama-2-7b-chat-hf",
    prompt="Write a Python function to calculate fibonacci numbers:",
    max_tokens=200,
    temperature=0.3
)

print(response.choices[0].text)

# ì±„íŒ… ìƒì„±
response = openai.ChatCompletion.create(
    model="meta-llama/Llama-2-7b-chat-hf",
    messages=[
        {"role": "user", "content": "Explain the concept of recursion"}
    ],
    max_tokens=200,
    temperature=0.7
)

print(response.choices[0].message.content)
```

### 3. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

```bash
# ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì‹¤ì‹œê°„ ì‘ë‹µ ë°›ê¸°
curl -X POST http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "prompt": "Write a story about a robot:",
    "max_tokens": 200,
    "stream": true
  }'
```

## ğŸ›¡ï¸ ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

### 1. ì•¡ì„¸ìŠ¤ ì œì–´

- Backend.AI ì•¡ì„¸ìŠ¤ í‚¤/ì‹œí¬ë¦¿ í‚¤ ë³´ì•ˆ ê´€ë¦¬
- HuggingFace í† í°ì„ í™˜ê²½ë³€ìˆ˜ë¡œë§Œ ì „ë‹¬
- í•„ìš”ì‹œ Private ëª¨ë¸ ì‚¬ìš©ìœ¼ë¡œ ë°ì´í„° ë³´ì•ˆ ê°•í™”

### 2. ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆ

```bash
# vLLM ì„œë²„ë¥¼ ë‚´ë¶€ ë„¤íŠ¸ì›Œí¬ì—ë§Œ ë…¸ì¶œ
./deploy-vllm-model.sh -p 8000 model-name  # ê¸°ë³¸ í¬íŠ¸ 8000
```

### 3. ë¦¬ì†ŒìŠ¤ ê²©ë¦¬

- Backend.AIì˜ ì„¸ì…˜ ê²©ë¦¬ ê¸°ëŠ¥ í™œìš©
- GPU ë¦¬ì†ŒìŠ¤ë¥¼ ì„¸ì…˜ë³„ë¡œ ì™„ì „ ë¶„ë¦¬
- ë©”ëª¨ë¦¬ ë° CPU ë¦¬ì†ŒìŠ¤ ì œí•œ

## ğŸ“š ì¶”ê°€ ë¦¬ì†ŒìŠ¤

### ë¬¸ì„œ
- [Backend.AI ê³µì‹ ë¬¸ì„œ](https://docs.backend.ai/)
- [vLLM ê³µì‹ ë¬¸ì„œ](https://vllm.readthedocs.io/)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers)

### ì˜ˆì œ ëª¨ë¸
- [HuggingFace Model Hub](https://huggingface.co/models)
- [Meta Llama Models](https://huggingface.co/meta-llama)
- [Mistral AI Models](https://huggingface.co/mistralai)

### ë„êµ¬
- `deploy-vllm-model.sh` - ë©”ì¸ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸
- `vllm-examples.sh` - ì˜ˆì œ ë° ìœ í‹¸ë¦¬í‹° ë„êµ¬
- Backend.AI Web Console - ì›¹ ê¸°ë°˜ ê´€ë¦¬ ë„êµ¬

---

## ğŸ’¡ Best Practices

### ê°œë°œ í™˜ê²½
- ì‘ì€ ëª¨ë¸ë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸ í›„ í° ëª¨ë¸ë¡œ í™•ì¥
- `--dry-run` ì˜µì…˜ìœ¼ë¡œ ì„¤ì • ê²€ì¦ í›„ ë°°í¬
- ë””ë²„ê·¸ ëª¨ë“œë¡œ ë¬¸ì œ í•´ê²°

### í”„ë¡œë•ì…˜ í™˜ê²½
- ì ì ˆí•œ ë¦¬ì†ŒìŠ¤ í• ë‹¹ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
- ëª¨ë‹ˆí„°ë§ ë„êµ¬ë¡œ ì§€ì†ì ì¸ ìƒíƒœ í™•ì¸
- ë¡œë“œ ë°¸ëŸ°ì‹±ì„ ìœ„í•œ ë‹¤ì¤‘ ì„¸ì…˜ ë°°í¬

### ë¹„ìš© ìµœì í™”
- ì–‘ìí™” ê¸°ë²•ìœ¼ë¡œ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
- ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì„¸ì…˜ì€ ì¦‰ì‹œ ì¢…ë£Œ
- ë°°ì¹˜ ì²˜ë¦¬ë¡œ GPU í™œìš©ë¥  ê·¹ëŒ€í™”

---

## â“ ì§€ì› ë° ë¬¸ì˜

ë¬¸ì œê°€ ë°œìƒí•˜ê±°ë‚˜ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´:

1. **ìŠ¤í¬ë¦½íŠ¸ ë””ë²„ê·¸**: `--debug --verbose` ì˜µì…˜ ì‚¬ìš©
2. **ë¡œê·¸ í™•ì¸**: `./vllm-examples.sh logs session-name`
3. **GitHub Issues**: Backend.AI í”„ë¡œì íŠ¸ì— ì´ìŠˆ ë“±ë¡
4. **ì»¤ë®¤ë‹ˆí‹°**: Backend.AI ì»¤ë®¤ë‹ˆí‹° í¬ëŸ¼ ì°¸ì—¬

**Happy Model Serving! ğŸ¤–ğŸš€**