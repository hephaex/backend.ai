# KubeVirt Network Configuration for Backend.AI GPU VMs
# This file configures networking between Backend.AI Manager and GPU Agent VMs

---
# Network Attachment Definition for GPU VM Management Network
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: backend-ai-gpu-management-net
  namespace: backend-ai-gpu
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "backend-ai-gpu-management",
      "type": "bridge",
      "bridge": "br-gpu-mgmt",
      "isDefaultGateway": false,
      "ipMasq": false,
      "ipam": {
        "type": "host-local",
        "subnet": "10.244.100.0/24",
        "rangeStart": "10.244.100.10",
        "rangeEnd": "10.244.100.200",
        "routes": [
          {
            "dst": "0.0.0.0/0"
          }
        ],
        "gateway": "10.244.100.1"
      }
    }

---
# Network Attachment Definition for High-Performance GPU Workloads (SR-IOV)
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: backend-ai-gpu-sriov-net
  namespace: backend-ai-gpu
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "backend-ai-gpu-sriov",
      "type": "sriov",
      "deviceID": "158b",
      "vf": 0,
      "ipam": {
        "type": "host-local",
        "subnet": "192.168.100.0/24",
        "rangeStart": "192.168.100.10",
        "rangeEnd": "192.168.100.200",
        "routes": [
          {
            "dst": "192.168.0.0/16"
          }
        ],
        "gateway": "192.168.100.1"
      }
    }

---
# Service for Backend.AI Manager to discover GPU Agents
apiVersion: v1
kind: Service
metadata:
  name: backend-ai-manager-discovery
  namespace: backend-ai
  labels:
    app: backend-ai-manager
    component: discovery
spec:
  selector:
    app: backend-ai-manager
  ports:
  - name: manager-rpc
    port: 8080
    targetPort: 8080
  - name: manager-api
    port: 8081
    targetPort: 8081
  type: ClusterIP

---
# Endpoints to connect Backend.AI Manager with GPU Agent VMs
apiVersion: v1
kind: Endpoints
metadata:
  name: backend-ai-gpu-agents-endpoint
  namespace: backend-ai
subsets:
- addresses:
  # These will be dynamically populated by the VM IPs
  - ip: 10.244.100.10  # GPU Agent VM 001
    targetRef:
      kind: Pod  # This would reference the VM pod
      name: virt-launcher-backend-ai-gpu-agent-001
      namespace: backend-ai-gpu
  - ip: 10.244.100.11  # GPU Agent VM 002 (if scaling)
    targetRef:
      kind: Pod
      name: virt-launcher-backend-ai-gpu-agent-002
      namespace: backend-ai-gpu
  ports:
  - name: rpc
    port: 6011
    protocol: TCP
  - name: watcher
    port: 6009
    protocol: TCP

---
# Service to expose GPU Agent endpoints to Backend.AI Manager
apiVersion: v1
kind: Service
metadata:
  name: backend-ai-gpu-agents-endpoint
  namespace: backend-ai
  labels:
    app: backend-ai-agent
    component: gpu-compute-nodes
spec:
  ports:
  - name: rpc
    port: 6011
    targetPort: 6011
  - name: watcher
    port: 6009
    targetPort: 6009

---
# RBAC for cross-namespace communication
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: backend-ai-cross-namespace-access
rules:
- apiGroups: [""]
  resources: ["services", "endpoints"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["kubevirt.io"]
  resources: ["virtualmachines", "virtualmachineinstances"]
  verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: backend-ai-cross-namespace-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: backend-ai-cross-namespace-access
subjects:
- kind: ServiceAccount
  name: backend-ai-manager
  namespace: backend-ai
- kind: ServiceAccount
  name: default
  namespace: backend-ai-gpu

---
# ConfigMap for Manager to discover GPU Agents
apiVersion: v1
kind: ConfigMap
metadata:
  name: backend-ai-manager-gpu-discovery
  namespace: backend-ai
data:
  gpu-agents.yaml: |
    gpu_agent_discovery:
      enabled: true
      namespaces:
        - backend-ai-gpu
      service_name: backend-ai-gpu-agents-headless
      port: 6011
      discovery_interval: 30
      health_check_interval: 10
      
  manager-gpu-config.toml: |
    # Additional configuration for Manager to work with GPU VMs
    [resource-manager]
    gpu_vm_support = true
    cross_namespace_discovery = true
    
    [etcd]
    namespace = "backend.ai"
    endpoints = ["http://backend-ai-etcd:2379"]
    
    [scaling-groups.gpu-nodes]
    driver = "static"
    max_nodes = 10
    
    [resource.cuda]
    enabled = true
    allocation_mode = "fractional"